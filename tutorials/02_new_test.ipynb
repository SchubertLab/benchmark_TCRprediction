{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6800d39-300b-4975-85d4-4f22cb4f8ff1",
   "metadata": {},
   "source": [
    "# How to add a new test-case to this benchmark.\n",
    "\n",
    "Adding a new test is a bit more complicated, as the implementation depends on the metrics choosen by the developer. Therefore, we will here showcase the construction of the viral benchmark, from which developers can then adapt for their datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49902cd0-13fe-45d4-b00b-4accbb364874",
   "metadata": {},
   "source": [
    "## The AbstractTest-class\n",
    "The custom test class below is supported by the AbstractTest class. Feel free to overwrite any element if it does not suit your purpose, but generally you can simply build a test inheriting from this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd68c772-3201-40e9-86fb-4467052f5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import abc\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tcr_benchmark.utils.config as config\n",
    "from tcr_benchmark.pp.datasets import get_dataset\n",
    "\n",
    "\n",
    "class AbstractTest(abc.ABC):\n",
    "    def __init__(self, name, path_out):\n",
    "        \"\"\"\n",
    "        Abstract test class for automatic testing.\n",
    "        :param name: str, name of the test\n",
    "        :param path_out: str, folder where results will be stored\n",
    "        \"\"\"\n",
    "        self.ds_name = name  # Name of the dataset (e.g. viral)\n",
    "\n",
    "        self.df_base_data = get_dataset(self.ds_name)  # load the base data for this test (see also tcr_benchmark.pp.datasets)\n",
    "\n",
    "        self.path_out = f\"{config.path_results}\" if path_out is None else path_out  # Create an output path, if none is provided\n",
    "        os.makedirs(self.path_out, exist_ok=True)\n",
    "\n",
    "        self.test_settings = {}\n",
    "\n",
    "    def run_tests(self, predictor, name=None, config_predictor=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Run the prediction based on the function you will provide (e.g. forming TCR-epitope pairs) and saves the output\n",
    "        prediction = self.run_prediction(predictor, config_predictor)\n",
    "        prediction.to_csv(f\"{self.path_out}/predictions_{name}_{self.ds_name}.csv\")\n",
    "\n",
    "        # Warns you if there are NaNs in the prediction. This occurs, when the predictor cannot handle the input correctly. E.g. to long sequences, missing information, unknown categories\n",
    "        mask_nans = prediction[\"Score\"].isna()\n",
    "        if mask_nans.sum() > 0:\n",
    "            warnings.warn(f\"Filtering {np.sum(mask_nans)} elements for {name} due to NaN prediction.\", stacklevel=-1)\n",
    "            prediction = prediction[~mask_nans]\n",
    "\n",
    "        # You can specify different tests in self.test_settings. All of them will be run and the saved to the results folder\n",
    "        results = []\n",
    "        for metric_type, test_func in self.test_settings.items():\n",
    "            df_tmp = test_func(prediction)\n",
    "            df_tmp[\"Metric_Type\"] = metric_type\n",
    "            results.append(df_tmp)\n",
    "        results = pd.concat(results)\n",
    "\n",
    "        # Compute the average and weighted average (by test support) of the metrics\n",
    "        groups = []\n",
    "        supports = []\n",
    "        metrics = []\n",
    "        values = []\n",
    "        datasets = []\n",
    "        types = []\n",
    "        for i, row in results[[\"Metric\", \"Metric_Type\"]].drop_duplicates().iterrows():\n",
    "            m = row[\"Metric\"]\n",
    "            c = row[\"Metric_Type\"]\n",
    "            df_tmp = results[(results[\"Metric\"] == m) & (results[\"Group\"] != \"full_data\")]\n",
    "            average = df_tmp[\"Value\"].mean()\n",
    "            weighted = (df_tmp[\"Value\"] * df_tmp[\"Support\"] / df_tmp[\"Support\"].sum()).sum()\n",
    "            groups += [\"Average\", \"WeightedAverage\"]\n",
    "            supports += [len(df_tmp)] * 2\n",
    "            metrics += [m] * 2\n",
    "            values += [average, weighted]\n",
    "            datasets += [\"All\"] * 2\n",
    "            types += [c] * 2\n",
    "        results_avg = pd.DataFrame({\n",
    "            \"Group\": groups,\n",
    "            \"Support\": supports,\n",
    "            \"Metric\": metrics,\n",
    "            \"Value\": values,\n",
    "            \"Dataset\": datasets,\n",
    "            \"Metric_Type\": types,\n",
    "        })\n",
    "        results = pd.concat([results, results_avg])\n",
    "\n",
    "        # Last formating and saving the output\n",
    "        results = results.reset_index(drop=True)\n",
    "        results[\"Method\"] = name\n",
    "        results = results[[\"Method\", \"Dataset\", \"Group\", \"Support\", \"Metric_Type\", \"Metric\", \"Value\"]]\n",
    "        results.to_csv(f\"{self.path_out}/results_{name}_{self.ds_name}.csv\")\n",
    "        return results\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def run_prediction(self, prediction_func, config_predictor):\n",
    "        \"\"\"\n",
    "        To reduce computational load, predictions are conducted once in this function covering all test settings.\n",
    "        :param config_predictor:\n",
    "        :param prediction_func: function, that receives a pd.DataFrame, and returns the dataframe with a binding score\n",
    "        :return: pd.DataFrame, containing TCR-epitope pairs, binding label, and prediction score\n",
    "        \"\"\"\n",
    "        # This function, you will need to implement. This mainly takes care of preprocessing the data into the right format, forming all required TCR-Epitope pairs, and running the prediction\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_results(self, results):\n",
    "        \"\"\"\n",
    "        Stores the results to disk.\n",
    "        :param results: dict {name, results as pd.DataFrame} containin the results of the individual tests\n",
    "        :return: writes results to \"{path_results}/{dataset_name}_{test_name}.csv\"\n",
    "        \"\"\"\n",
    "        for test_name, result in results.items():\n",
    "            path_res = f\"{self.path_out}/{self.ds_name}_{test_name}.csv\"\n",
    "            result.to_csv(path_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769822b-506d-4f27-8977-db072dbb6c87",
   "metadata": {},
   "source": [
    "## The actual test\n",
    "The viral test case. This is just an example. You will need to adapt this for your own custom test. Note, that depending on your test scenario you will want to define negatives differently (in `run_prediciton`) or use different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314fa265-f6d1-478c-8965-712f709ee4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tcr_benchmark.eval.abstractTests import AbstractTest\n",
    "import tcr_benchmark.eval.metrics as metrics\n",
    "\n",
    "class ViralTest(AbstractTest):\n",
    "    def __init__(self, path_out):\n",
    "        \"\"\"\n",
    "        :param path_out: Path to the output CSV file to which the results will be stored. If None, the path will be constructed from the name (see below).\n",
    "        \"\"\"\n",
    "        super().__init__(\"viral\", path_out)  # Choose a name for your benchmark here\n",
    "\n",
    "        # Specify the different test functions to run and a name for them\n",
    "        self.test_settings = {\n",
    "            \"MPS\": self.run_multiple_peptide_selection_test,  # In this test setting, you have 1 TCR and X options of epitopes from which you want to choose the highest one. Exactly one of the epitopes should be annotated as binding (Label = 1)\n",
    "            \"TTP\": self.run_tcr_peptide_pairing_test,  # In this test setting, you have pairs of TCRs and epitopes annotated as binders (Label=1) or non-binders (Label=0). Note, in order to calculate classification metrics such as AUC you will need at least one positive and one negative pair\n",
    "        }\n",
    "\n",
    "        #\n",
    "        self.test_data = None\n",
    "\n",
    "    def run_prediction(self, predictor, config_predictor):\n",
    "        \"\"\"\n",
    "        This function performs the prediction for the whole data. It is useful to not run the prediction separately for each test to safe computational resources\n",
    "        :param predictor: a function that follows the interface of these test (see tutorials/01_ne_method.ipynb)\n",
    "        :param config_predictor: kwargs passed to this method, (e.g. to select different model choices)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # For MPS we want to predict between every combination of TCR and epitopes. We therefore constract a dataframe with all combinations. \n",
    "        # The input data are TCR-Epitope pairs that bind\n",
    "        epitope_mhcs = self.df_base_data[[\"Epitope\", \"MHC\"]].drop_duplicates().values  # Select Unique MHC-Epitope combinations\n",
    "        data_full = []\n",
    "        for epitope, mhc in epitope_mhcs:\n",
    "            # For all TCRs of the dataframe, add this specific MHC-Epitope\n",
    "            df_tmp = self.df_base_data.copy()\n",
    "            df_tmp[\"Epitope\"] = epitope\n",
    "            df_tmp[\"MHC\"] = mhc\n",
    "            data_full.append(df_tmp)\n",
    "        data_full = pd.concat(data_full)\n",
    "        data_full = pd.merge(data_full, self.df_base_data, how=\"left\", indicator=\"Label\")  # Will have Label == both, if the columns of the new data also occur in the positive binding dataframe\n",
    "        data_full[\"Label\"] = np.where(data_full.Label == \"both\", 1, 0)  # If this is the case, the label will be 1 (== binders) otherwise 0 (== non-binders)\n",
    "\n",
    "        prediction = predictor(data_full, **config_predictor)  # We now use the predictor to provide a \"Score\" for all these combinations\n",
    "        return prediction\n",
    "\n",
    "    def run_multiple_peptide_selection_test(self, prediction):\n",
    "        # Warning due to NaN predictions. This could occur if information required for the predictors are missing, have wrong sequence length, or are not allowed.\n",
    "        if np.sum(prediction[\"Score\"].notna()) != len(prediction):\n",
    "            warnings.warn(f\"Filter out {np.sum(prediction['Score'].isna())} Predictions due to NaN values. \"\n",
    "                          f\"Metrics invalid\")\n",
    "            prediction = prediction[prediction[\"Score\"].isna()]\n",
    "\n",
    "        # Currently, the prediction results are in long-format (TCR-epitope pairs), but the metrics work on broad format (TCR - epitope1, epitope2, ..., epitopeN)\n",
    "        # So let's pivot the table, and create labels, which of these predictions is correct\n",
    "        prediction[\"Epitope_MHC\"] = prediction[\"Epitope\"] + \"_\" + prediction[\"MHC\"]\n",
    "        prediction = prediction.drop(columns=[\"Epitope\", \"MHC\"])\n",
    "        labels = prediction.pivot_table(index=[\"CDR3_alpha\", \"V_alpha\", \"J_alpha\", \"CDR3_beta\", \"V_beta\", \"J_beta\"],\n",
    "                                        columns=[\"Epitope_MHC\"], values=\"Label\")\n",
    "        prediction = prediction.pivot_table(\n",
    "            index=[\"CDR3_alpha\", \"V_alpha\", \"J_alpha\", \"CDR3_beta\", \"V_beta\", \"J_beta\"],\n",
    "            columns=[\"Epitope_MHC\"], values=\"Score\")\n",
    "\n",
    "        epitopes = prediction.columns\n",
    "        labels = labels[epitopes]\n",
    "        labels = labels.apply(lambda x: \"\".join([x[el] * el for el in epitopes]), axis=1)\n",
    "\n",
    "        scores = metrics.calculated_rank_metrics(labels, prediction, labels, [1, 3, 5, 8])  # Rank metrics are based on the ordering of prediction scores, the list indicates what Ks to choose for R@K\n",
    "        scores[\"Dataset\"] = \"Viral\"  # Providing a name makes it easier to track from which dataset the csv file originates\n",
    "        return scores\n",
    "\n",
    "    def run_tcr_peptide_pairing_test(self, prediction):\n",
    "        # This is a bit more straight forward. Here, we calculate classical classification metrics on the dataset. We just need Label and Score, and the name of the epitope to form groups.\n",
    "        scores = metrics.calculate_score_metrics(prediction[\"Label\"], prediction[\"Score\"], prediction[\"Epitope\"])  # AUC and APS\n",
    "        scores_class = metrics.calculate_classification_metrics(prediction[\"Label\"], prediction[\"Score\"],\n",
    "                                                                prediction[\"Epitope\"])  # F1-Score, Accuracy, Precision, Recall\n",
    "        scores = pd.concat([scores, scores_class])\n",
    "        scores[\"Dataset\"] = \"Viral\"\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfc179-4cbf-452d-8ee0-0ef733b2e668",
   "metadata": {},
   "source": [
    "## 03. Running a test\n",
    "You can now run the test on individual or all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adea461-048c-4ae2-9bd3-78ef59144cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prediction_func(df_input, **kwargs):\n",
    "    \"\"\" See notebook 01_new_method. \"\"\"\n",
    "    df_output = df_input.copy()\n",
    "    np.random.seed(0)\n",
    "    df_output[\"Score\"] = np.random.rand(len(df_output))\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0fd24d-a99e-456c-b7d4-3de7484a0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ViralTest(None)\n",
    "results_dataset = test.run_tests(prediction_func, 'dummy_predictor', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff8c38-586e-479d-9399-14f804a5dd8e",
   "metadata": {},
   "source": [
    "To conduct the test on all ePytope-TCR predictors, you could loop over all methods, or use the provided configs to adapt `tcr_benchmark.study.benchmark_alternatives`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2507f6de-8594-4082-9b52-24d46ab4e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imrex \n",
      "titan 1.0.0\n",
      "tcellmatch \n",
      "stapler \n",
      "ergo-ii \n",
      "pmtnet \n",
      "epitcr \n",
      "atm-tcr \n",
      "attntap \n",
      "teim \n",
      "bertrand \n",
      "ergo-i \n",
      "teinet \n",
      "panpep \n",
      "dlptcr \n",
      "tulip-tcr \n",
      "itcep \n",
      "nettcr 2.2\n",
      "mixtcrpred \n",
      "tcrgp \n"
     ]
    }
   ],
   "source": [
    "from epytope.TCRSpecificityPrediction import TCRSpecificityPredictorFactory\n",
    "for name, version in TCRSpecificityPredictorFactory.available_methods().items():\n",
    "    print(name, \",\".join(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b3bd1-cf9d-4cc8-867f-60bfd2d9cd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
